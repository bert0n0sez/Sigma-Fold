{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839cb11b",
   "metadata": {},
   "source": [
    "# 04 Transformer Training\n",
    "\n",
    "В этом ноутбуке будет осуществлена тренировка нейросетевой архитектуры на основе трансформера (esm_classifier) для задачи предсказания вторичной структуры белка по аминокислотным последовательностям.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6eca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Загрузка предобработанных данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"/trinity/home/e.bulavko/a.khokhlov/data/processed\"\n",
    "\n",
    "Xtrain = np.load(f'{base_path}/X_train.npy')\n",
    "ytrain = np.load(f'{base_path}/y_train.npy')\n",
    "\n",
    "Xval = np.load(f'{base_path}/X_val.npy')\n",
    "yval = np.load(f'{base_path}/y_val.npy')\n",
    "\n",
    "Xtest = np.load(f'{base_path}/X_test.npy')\n",
    "ytest = np.load(f'{base_path}/y_test.npy')\n",
    "\n",
    "mask_train = np.load(f'{base_path}/mask_train.npy')\n",
    "mask_val = np.load(f'{base_path}/mask_val.npy')\n",
    "mask_test = np.load(f'{base_path}/mask_test.npy')\n",
    "\n",
    "class_weights = np.load(f'{base_path}/class_weights.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2320428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_data(X, y, mask, factor=4):\n",
    "    # new length будет в factor раз меньше\n",
    "    new_length = X.shape[0] // factor\n",
    "\n",
    "    X_subsampled = X[:new_length]\n",
    "    y_subsampled = y[:new_length]\n",
    "    mask_subsampled = mask[:new_length]\n",
    "\n",
    "    return X_subsampled, y_subsampled, mask_subsampled\n",
    "\n",
    "Xtrain, ytrain, mask_train = subsample_data(Xtrain, ytrain, mask_train, factor=1)\n",
    "Xval, yval, mask_val = subsample_data(Xval, yval, mask_val, factor=1)\n",
    "Xtest, ytest, mask_test = subsample_data(Xtest, ytest, mask_test, factor=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0bca9e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Кастомный Dataset для PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e11c674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, X, y, mask):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.mask = mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input': torch.LongTensor(self.X[idx]),\n",
    "            'target': torch.LongTensor(self.y[idx]),\n",
    "            'mask': torch.BoolTensor(self.mask[idx]),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642fb247",
   "metadata": {},
   "source": [
    "## DataLoader'ы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataset = ProteinDataset(Xtrain, ytrain, mask_train)\n",
    "val_dataset = ProteinDataset(Xval, yval, mask_val)\n",
    "test_dataset = ProteinDataset(Xtest, ytest, mask_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print('Данные загружены')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c48ee",
   "metadata": {},
   "source": [
    "## Определение архитектуры трансформера (esm_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04f7b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels, d_model=128, nhead=8, num_layers=2, dim_feedforward=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        key_padding_mask = ~mask\n",
    "        x = self.transformer(x, src_key_padding_mask=key_padding_mask)\n",
    "        x = x.transpose(0, 1)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7479e2c0",
   "metadata": {},
   "source": [
    "Инициализатор\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5152ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_normal(model, std=0.02):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param, mean=0.0, std=std)\n",
    "        elif 'bias' in name:\n",
    "            nn.init.constant_(param, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b7671d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Подготовка к обучению: функция потерь, модель, оптимизатор\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfee45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_labels = 4\n",
    "vocab_size = 21\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ESMClassifier(vocab_size, num_labels).to(device)\n",
    "init_weights_normal(model) #добавили инициализацию\n",
    "weights = np.concatenate(([0.0], class_weights))\n",
    "#loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float32, device=device), ignore_index=0)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "print('Модель подготовлена')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4b7609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Цикл обучения с валидацией\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53aab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время обработки одного батча: 0.85 секунд\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model.train()\n",
    "start = time.time()\n",
    "for i, batch in enumerate(train_loader):\n",
    "    inputs = batch['input'].to(device)\n",
    "    targets = batch['target'].to(device)\n",
    "    mask = batch['mask'].to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs, mask)\n",
    "    outputs = outputs.view(-1, num_labels)\n",
    "    targets_flat = targets.view(-1).long()\n",
    "    loss = loss_fn(outputs, targets_flat)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) \n",
    "    optimizer.step()\n",
    "\n",
    "    if i == 0:  # только одна итерация для оценки времени\n",
    "        break\n",
    "end = time.time()\n",
    "print(f\"Время обработки одного батча: {end - start:.2f} секунд\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb3b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "best_val_f1 = 0\n",
    "\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch['input'].to(device)\n",
    "        targets = batch['target'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, mask)\n",
    "        outputs = outputs.view(-1, num_labels)\n",
    "        targets_flat = targets.view(-1)\n",
    "        loss = loss_fn(outputs, targets_flat)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) #добавил от безысходности\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch['input'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            outputs = model(inputs, mask)\n",
    "            preds = outputs.argmax(dim=-1)\n",
    "            for i in range(inputs.size(0)):\n",
    "                seq_mask = mask[i].cpu().numpy()\n",
    "                all_preds.extend(preds[i][seq_mask].cpu().numpy())\n",
    "                all_targets.extend(targets[i][seq_mask].cpu().numpy())\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train loss: {train_loss:.4f}')\n",
    "    print(classification_report(all_targets, all_preds, digits=3))\n",
    "\n",
    "    f1 = classification_report(all_targets, all_preds, output_dict=True, zero_division=0)['weighted avg']['f1-score']\n",
    "    if f1 > best_val_f1:  # Изменил >= на > для более строгого критерия\n",
    "        best_val_f1 = f1\n",
    "        patience_counter = 0  # ← сбросить счетчик\n",
    "        torch.save(model.state_dict(), 'best_esm_classifier.pth')\n",
    "        print('Model saved!')\n",
    "    else:\n",
    "        patience_counter += 1  # ← увеличить счетчик\n",
    "        if patience_counter >= patience:  # ← проверить лимит\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "print('Best validation F1:', best_val_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0df41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Тестирование лучшей модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032c4507",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_esm_classifier.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbest_esm_classifier.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      2\u001b[39m model.eval()\n\u001b[32m      3\u001b[39m all_test_preds = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python\\Lib\\site-packages\\torch\\serialization.py:1484\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1482\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1486\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1487\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1488\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1489\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python\\Lib\\site-packages\\torch\\serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\python\\Lib\\site-packages\\torch\\serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'best_esm_classifier.pth'"
     ]
    }
   ],
   "source": [
    "\n",
    "if os.path.exists('best_esm_classifier.pth'):\n",
    "    model.load_state_dict(torch.load('best_esm_classifier.pth'))\n",
    "    print(\"Загружена лучшая модель.\")\n",
    "else:\n",
    "    print(\"Внимание: Лучшая модель не найдена (возможно, обучение не сошлось). Тестирование будет на текущих весах.\")\n",
    "\n",
    "model.eval()\n",
    "all_test_preds = []\n",
    "all_test_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = batch['input'].to(device)\n",
    "        targets = batch['target'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        outputs = model(inputs, mask)\n",
    "        preds = outputs.argmax(dim=-1)\n",
    "        for i in range(inputs.size(0)):\n",
    "            seq_mask = mask[i].cpu().numpy()\n",
    "            all_test_preds.extend(preds[i][seq_mask].cpu().numpy())\n",
    "            all_test_targets.extend(targets[i][seq_mask].cpu().numpy())\n",
    "\n",
    "print(classification_report(all_test_targets, all_test_preds, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8310e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
